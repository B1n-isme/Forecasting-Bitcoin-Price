{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Google Trends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing interval: 2023-03-12 to 2023-04-11...\n",
      "Attempt 1/10000: Fetching data from 2023-03-12 to 2023-04-11...\n",
      "Data from 2023-03-12 to 2023-04-11 saved successfully.\n",
      "Processing interval: 2023-04-12 to 2023-05-12...\n",
      "Attempt 1/10000: Fetching data from 2023-04-12 to 2023-05-12...\n",
      "Data from 2023-04-12 to 2023-05-12 saved successfully.\n",
      "Processing interval: 2023-05-13 to 2023-06-12...\n",
      "Attempt 1/10000: Fetching data from 2023-05-13 to 2023-06-12...\n",
      "Data from 2023-05-13 to 2023-06-12 saved successfully.\n",
      "Processing interval: 2023-06-13 to 2023-07-13...\n",
      "Attempt 1/10000: Fetching data from 2023-06-13 to 2023-07-13...\n",
      "Data from 2023-06-13 to 2023-07-13 saved successfully.\n",
      "Processing interval: 2023-07-14 to 2023-08-13...\n",
      "Attempt 1/10000: Fetching data from 2023-07-14 to 2023-08-13...\n",
      "Data from 2023-07-14 to 2023-08-13 saved successfully.\n",
      "Processing interval: 2023-08-14 to 2023-09-13...\n",
      "Attempt 1/10000: Fetching data from 2023-08-14 to 2023-09-13...\n",
      "Data from 2023-08-14 to 2023-09-13 saved successfully.\n",
      "Processing interval: 2023-09-14 to 2023-10-14...\n",
      "Attempt 1/10000: Fetching data from 2023-09-14 to 2023-10-14...\n",
      "Data from 2023-09-14 to 2023-10-14 saved successfully.\n",
      "Processing interval: 2023-10-15 to 2023-11-14...\n",
      "Attempt 1/10000: Fetching data from 2023-10-15 to 2023-11-14...\n",
      "Data from 2023-10-15 to 2023-11-14 saved successfully.\n",
      "Processing interval: 2023-11-15 to 2023-12-15...\n",
      "Attempt 1/10000: Fetching data from 2023-11-15 to 2023-12-15...\n",
      "Data from 2023-11-15 to 2023-12-15 saved successfully.\n",
      "Processing interval: 2023-12-16 to 2024-01-15...\n",
      "Attempt 1/10000: Fetching data from 2023-12-16 to 2024-01-15...\n",
      "Data from 2023-12-16 to 2024-01-15 saved successfully.\n",
      "Processing interval: 2024-01-16 to 2024-02-15...\n",
      "Attempt 1/10000: Fetching data from 2024-01-16 to 2024-02-15...\n",
      "Data from 2024-01-16 to 2024-02-15 saved successfully.\n",
      "Processing interval: 2024-02-16 to 2024-03-17...\n",
      "Attempt 1/10000: Fetching data from 2024-02-16 to 2024-03-17...\n",
      "Data from 2024-02-16 to 2024-03-17 saved successfully.\n",
      "Processing interval: 2024-03-18 to 2024-04-17...\n",
      "Attempt 1/10000: Fetching data from 2024-03-18 to 2024-04-17...\n",
      "Data from 2024-03-18 to 2024-04-17 saved successfully.\n",
      "Processing interval: 2024-04-18 to 2024-05-18...\n",
      "Attempt 1/10000: Fetching data from 2024-04-18 to 2024-05-18...\n",
      "Data from 2024-04-18 to 2024-05-18 saved successfully.\n",
      "Processing interval: 2024-05-19 to 2024-06-18...\n",
      "Attempt 1/10000: Fetching data from 2024-05-19 to 2024-06-18...\n",
      "Data from 2024-05-19 to 2024-06-18 saved successfully.\n",
      "Processing interval: 2024-06-19 to 2024-07-19...\n",
      "Attempt 1/10000: Fetching data from 2024-06-19 to 2024-07-19...\n",
      "Data from 2024-06-19 to 2024-07-19 saved successfully.\n",
      "Processing interval: 2024-07-20 to 2024-08-19...\n",
      "Attempt 1/10000: Fetching data from 2024-07-20 to 2024-08-19...\n",
      "Data from 2024-07-20 to 2024-08-19 saved successfully.\n",
      "Processing interval: 2024-08-20 to 2024-09-19...\n",
      "Attempt 1/10000: Fetching data from 2024-08-20 to 2024-09-19...\n",
      "Data from 2024-08-20 to 2024-09-19 saved successfully.\n",
      "Processing interval: 2024-09-20 to 2024-10-20...\n",
      "Attempt 1/10000: Fetching data from 2024-09-20 to 2024-10-20...\n",
      "Data from 2024-09-20 to 2024-10-20 saved successfully.\n",
      "Processing interval: 2024-10-21 to 2024-11-16...\n",
      "Attempt 1/10000: Fetching data from 2024-10-21 to 2024-11-16...\n",
      "Data from 2024-10-21 to 2024-11-16 saved successfully.\n",
      "Data fetching completed. All data saved to bitcoin_daily_google_trends.csv.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "from pytrends.request import TrendReq\n",
    "import os\n",
    "\n",
    "# Function to generate daily intervals\n",
    "def generate_daily_intervals(start_date, end_date):\n",
    "    intervals = []\n",
    "    while start_date < end_date:\n",
    "        interval_end = min(start_date + timedelta(days=30), end_date)\n",
    "        intervals.append((start_date.strftime(\"%Y-%m-%d\"), interval_end.strftime(\"%Y-%m-%d\")))\n",
    "        start_date = interval_end + timedelta(days=1)\n",
    "    return intervals\n",
    "\n",
    "# Function to rescale data between overlapping intervals\n",
    "def rescale_data(existing_data, new_data):\n",
    "    overlap = existing_data.index.intersection(new_data.index)\n",
    "    if not overlap.empty:\n",
    "        # Calculate scaling factor using overlapping days\n",
    "        scale_factor = existing_data.loc[overlap].mean() / new_data.loc[overlap].mean()\n",
    "        # Rescale new data\n",
    "        new_data *= scale_factor\n",
    "        print(f\"Rescaled new data by factor: {scale_factor}\")\n",
    "    return new_data\n",
    "\n",
    "# Function to fetch data with backoff logic\n",
    "def fetch_with_backoff(pytrends, keyword, interval, geo, retries=10000):\n",
    "    delay = 0  # Initial delay in seconds\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            print(f\"Attempt {attempt + 1}/{retries}: Fetching data from {interval[0]} to {interval[1]}...\")\n",
    "            pytrends.build_payload([keyword], timeframe=f\"{interval[0]} {interval[1]}\", geo=geo)\n",
    "            data = pytrends.interest_over_time()\n",
    "            \n",
    "            # Ensure data is not empty\n",
    "            if not data.empty:\n",
    "                return data\n",
    "            else:\n",
    "                print(f\"No data available for {interval[0]} to {interval[1]}.\")\n",
    "                return pd.DataFrame()  # Return empty DataFrame\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching data for {interval}: {e}\")\n",
    "            if attempt < retries - 1:\n",
    "                print(f\"Retrying in {delay} seconds...\")\n",
    "                time.sleep(delay)\n",
    "                delay *= 2  # Exponential backoff\n",
    "            else:\n",
    "                print(\"Max retries reached. Skipping this interval.\")\n",
    "                return pd.DataFrame()  # Return empty DataFrame if max retries reached\n",
    "\n",
    "# Fetch Google Trends data with improvements\n",
    "def fetch_google_trends_data(keyword, start_date, end_date, geo=\"US\", output_file=\"google_trends_data.csv\"):\n",
    "    pytrends = TrendReq(hl='en-US', tz=360, timeout=(10, 25))\n",
    "    \n",
    "    # Generate daily intervals (30 days max)\n",
    "    intervals = generate_daily_intervals(start_date, end_date)\n",
    "\n",
    "    if os.path.exists(output_file):\n",
    "        existing_data = pd.read_csv(output_file, index_col=0, parse_dates=True)\n",
    "    else:\n",
    "        existing_data = pd.DataFrame()\n",
    "\n",
    "    for interval in intervals:\n",
    "        print(f\"Processing interval: {interval[0]} to {interval[1]}...\")\n",
    "        data = fetch_with_backoff(pytrends, keyword, interval, geo)\n",
    "        \n",
    "        if not data.empty:\n",
    "            data = data.drop(columns=['isPartial'], errors='ignore')\n",
    "            data.index = pd.to_datetime(data.index)\n",
    "\n",
    "            # Combine and rescale data\n",
    "            data = rescale_data(existing_data, data)\n",
    "            existing_data = pd.concat([existing_data, data])\n",
    "            existing_data.sort_index(inplace=True)\n",
    "            existing_data.to_csv(output_file)\n",
    "            print(f\"Data from {interval[0]} to {interval[1]} saved successfully.\")\n",
    "        else:\n",
    "            print(f\"Skipping interval {interval[0]} to {interval[1]} due to fetch issues.\")\n",
    "\n",
    "# Main script\n",
    "if __name__ == \"__main__\":\n",
    "    # Define parameters\n",
    "    keyword = \"bitcoin\"\n",
    "    start_date = datetime(2023, 3, 12)\n",
    "    end_date = datetime(2024, 11, 16)\n",
    "    geo = ''  # Leave empty for worldwide data\n",
    "    output_file = \"../data/raw/bitcoin_daily_google_trends.csv\"\n",
    "\n",
    "    # Fetch data\n",
    "    fetch_google_trends_data(keyword, start_date, end_date, geo, output_file)\n",
    "    print(f\"Data fetching completed. All data saved to {output_file}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicates removed. Cleaned data saved to bitcoin_daily_google_trends.csv.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Function to remove duplicate dates\n",
    "def remove_duplicates(csv_file, output_file):\n",
    "    # Load the data\n",
    "    data = pd.read_csv(csv_file, index_col=0, parse_dates=True)\n",
    "    \n",
    "    # Sort the data by index (date)\n",
    "    data.sort_index(inplace=True)\n",
    "    \n",
    "    # Drop duplicate indices, keeping only the first occurrence\n",
    "    data = data[~data.index.duplicated(keep='first')]\n",
    "    \n",
    "    # Save the cleaned data back to the CSV file\n",
    "    data.to_csv(output_file)\n",
    "    print(f\"Duplicates removed. Cleaned data saved to {output_file}.\")\n",
    "\n",
    "# Specify file paths\n",
    "input_file = \"../data/raw/bitcoin_daily_google_trends.csv\"  # Input file with duplicates\n",
    "output_file = \"../data/processed/bitcoin_daily_google_trends.csv\"  # Output file without duplicates\n",
    "\n",
    "# Remove duplicates\n",
    "remove_duplicates(input_file, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rescaled data saved to bitcoin_rescaled_google_trends.csv.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Function to rescale data between overlapping intervals\n",
    "def rescale_data(data):\n",
    "    rescaled_data = pd.DataFrame()\n",
    "    intervals = data.groupby(data.index.year)  # Group data by year for easier processing\n",
    "    previous_interval = None\n",
    "\n",
    "    for year, interval in intervals:\n",
    "        if previous_interval is not None:\n",
    "            # Identify overlap\n",
    "            overlap = previous_interval.index.intersection(interval.index)\n",
    "            if not overlap.empty:\n",
    "                # Calculate scaling factor\n",
    "                scale_factor = previous_interval.loc[overlap].mean() / interval.loc[overlap].mean()\n",
    "                interval *= scale_factor\n",
    "                print(f\"Year {year}: Rescaled interval by factor {scale_factor:.4f}\")\n",
    "\n",
    "        # Add the interval to the rescaled data\n",
    "        rescaled_data = pd.concat([rescaled_data, interval])\n",
    "        previous_interval = interval\n",
    "\n",
    "    rescaled_data.sort_index(inplace=True)\n",
    "    return rescaled_data\n",
    "\n",
    "# Load collected data\n",
    "input_file = \"../data/processed/bitcoin_daily_google_trends.csv\"\n",
    "output_file = \"../data/processed/bitcoin_daily_google_trends.csv\"\n",
    "data = pd.read_csv(input_file, index_col=0, parse_dates=True)\n",
    "\n",
    "# Rescale data\n",
    "rescaled_data = rescale_data(data)\n",
    "\n",
    "# Save rescaled data\n",
    "rescaled_data.to_csv(output_file)\n",
    "print(f\"Rescaled data saved to {output_file}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date format updated with UTC. Saved to bitcoin_utc_google_trends.csv.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Function to add UTC time to date index\n",
    "def format_date_with_utc(input_file, output_file):\n",
    "    # Load the data\n",
    "    data = pd.read_csv(input_file, index_col=0, parse_dates=True)\n",
    "    \n",
    "    # Ensure the index is a datetime object with UTC timezone\n",
    "    data.index = data.index.tz_localize('UTC')\n",
    "    \n",
    "    # Save the updated data to a new CSV file\n",
    "    data.to_csv(output_file, date_format='%Y-%m-%d %H:%M:%S%z')\n",
    "    print(f\"Date format updated with UTC. Saved to {output_file}.\")\n",
    "\n",
    "# Specify file paths\n",
    "input_file = \"../data/processed/bitcoin_daily_google_trends.csv\"  # Input file without UTC time\n",
    "output_file = \"../data/processed/bitcoin_daily_google_trends.csv\"  # Output file with UTC time\n",
    "\n",
    "# Add UTC time to dates\n",
    "format_date_with_utc(input_file, output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Oil and Gold Price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n",
      "\n",
      "1 Failed download:\n",
      "['CL=F']: ReadTimeout(ReadTimeoutError(\"HTTPSConnectionPool(host='query2.finance.yahoo.com', port=443): Read timed out. (read timeout=10)\"))\n",
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    }
   ],
   "source": [
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "\n",
    "# Define symbols for oil and gold\n",
    "# For example, CL=F is the crude oil futures and GC=F is gold futures on Yahoo Finance\n",
    "oil_symbol = \"CL=F\"\n",
    "gold_symbol = \"GC=F\"\n",
    "\n",
    "# Fetch data for the past 10 years\n",
    "oil_data = yf.download(oil_symbol, start=\"2014-11-14\", end=\"2024-11-17\", interval=\"1d\")\n",
    "gold_data = yf.download(gold_symbol, start=\"2014-11-14\", end=\"2024-11-17\", interval=\"1d\")\n",
    "\n",
    "# Save to CSV or analyze directly\n",
    "oil_data.to_csv(\"../data/raw/oil_prices.csv\")\n",
    "gold_data.to_csv(\"../data/raw/gold_prices.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            Gold_Close  Oil_Close\n",
      "Date                                             \n",
      "2014-11-14 00:00:00+00:00  1185.000000  75.820000\n",
      "2014-11-15 00:00:00+00:00  1185.000000  75.820000\n",
      "2014-11-16 00:00:00+00:00  1185.000000  75.820000\n",
      "2014-11-17 00:00:00+00:00  1183.000000  75.639999\n",
      "2014-11-18 00:00:00+00:00  1196.699951  74.610001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/pf/4hxrldrx2g7g5x19dwlvk36w0000gn/T/ipykernel_87298/1861915078.py:24: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  gold_data_cleaned = gold_data_cleaned.fillna(method='ffill')\n",
      "/var/folders/pf/4hxrldrx2g7g5x19dwlvk36w0000gn/T/ipykernel_87298/1861915078.py:25: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  oil_data_cleaned = oil_data_cleaned.fillna(method='ffill')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "\n",
    "# Load the cleaned data\n",
    "gold_data = pd.read_csv(\"../data/raw/gold_prices.csv\", parse_dates=[\"Date\"], index_col=\"Date\")\n",
    "oil_data = pd.read_csv(\"../data/raw/oil_prices.csv\", parse_dates=[\"Date\"], index_col=\"Date\")\n",
    "\n",
    "# Retain only the Close prices\n",
    "gold_data_cleaned = gold_data[['Close']].rename(columns={'Close': 'Gold_Close'})\n",
    "oil_data_cleaned = oil_data[['Close']].rename(columns={'Close': 'Oil_Close'})\n",
    "\n",
    "# Create a full date range for the data\n",
    "full_date_range = pd.date_range(start=gold_data_cleaned.index.min(), end=gold_data_cleaned.index.max())\n",
    "\n",
    "# Reindex to the full date range\n",
    "gold_data_cleaned = gold_data_cleaned.reindex(full_date_range)\n",
    "oil_data_cleaned = oil_data_cleaned.reindex(full_date_range)\n",
    "\n",
    "# Ensure the index is named 'Date'\n",
    "gold_data_cleaned.index.name = 'Date'\n",
    "oil_data_cleaned.index.name = 'Date'\n",
    "\n",
    "# Step 1: Forward Fill for Edge Cases\n",
    "gold_data_cleaned = gold_data_cleaned.fillna(method='ffill')\n",
    "oil_data_cleaned = oil_data_cleaned.fillna(method='ffill')\n",
    "\n",
    "# Step 2: Linear Interpolation for Small Gaps\n",
    "gold_data_cleaned = gold_data_cleaned.interpolate(method='linear')\n",
    "oil_data_cleaned = oil_data_cleaned.interpolate(method='linear')\n",
    "\n",
    "# Step 3: ARIMA-Based Imputation for Large Gaps\n",
    "def arima_impute(series):\n",
    "    if series.isnull().sum() > 0:  # Apply ARIMA only if there are still missing values\n",
    "        model = ARIMA(series, order=(1, 1, 1))  # Adjust (p, d, q) based on data\n",
    "        fitted_model = model.fit()\n",
    "        series = series.fillna(fitted_model.fittedvalues)\n",
    "    return series\n",
    "\n",
    "gold_data_cleaned['Gold_Close'] = arima_impute(gold_data_cleaned['Gold_Close'])\n",
    "oil_data_cleaned['Oil_Close'] = arima_impute(oil_data_cleaned['Oil_Close'])\n",
    "\n",
    "# Combine the gold and oil data\n",
    "combined_data = gold_data_cleaned.join(oil_data_cleaned, how='inner')\n",
    "\n",
    "# Save the combined DataFrame to a CSV file\n",
    "combined_data.to_csv(\"../data/processed/gold_oil_prices.csv\")\n",
    "\n",
    "# Display the result\n",
    "print(combined_data.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "webscraping",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
