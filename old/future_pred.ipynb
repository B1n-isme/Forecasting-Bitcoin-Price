{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here is what i have achieved, memorized it:\n",
    "# 1. I have a 'synthetic_features' df stored 7-days forecasted of non-technical indicators\n",
    "# 2. I have 'future_predictions_df' df stored prediction of next first day of 4 final arima-garch+lstm model and their ensemble prediction (total 5 prediction)\n",
    "# 3. I have a 'high_low_vol_forecast\" df stored 7-days forecasted of high, low, volume of bitcoin price\n",
    "# Now I want you to help me implement our future prediction main workflow pipeline:\n",
    "# 1. for the future prediction of first date, use the calculate_technical_indicators module above (I have stored it in a py file) to calculate the technical indicator from the high, low, volume, and the ensemble prediction from future_predictions_df\n",
    "# 2. now we have full raw features (synthetics_features + technical_features) for the first date, use a scaler (loaded from saved of train_data) to transform those raw features.\n",
    "# 3. Next, fit those PCs to arima_forecast, garch_volatility_forecast to find the arima_garch_forecast (i already have arimax_model.pkl, garch_model.pkl from train to load). Then, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forecast Synthetic Non-Technical Features:\n",
    "\n",
    "# Use models like ARIMA or VAR to predict non-technical indicators (e.g., sentiment, blockchain metrics) for the 7-day horizon.\n",
    "# Recursive Loop (1 Day at a Time):\n",
    "\n",
    "# Day 1:\n",
    "# Forecast btc_price using ARIMA-GARCH (with PCs as exog_vars) + LSTM adjustment.\n",
    "# Use the forecasted btc_price to calculate Day 2's technical indicators.\n",
    "# Day 2–7:\n",
    "# Repeat the process:\n",
    "# Forecast btc_price for the next day.\n",
    "# Recalculate technical indicators based on the latest forecasted price.\n",
    "# Update the PCA-reduced feature set for the next prediction.\n",
    "# Combine Predictions:\n",
    "\n",
    "# At each step, compute:\n",
    "# Final BTC Price\n",
    "# =\n",
    "# ARIMA-GARCH Forecast\n",
    "# +\n",
    "# Δ\n",
    "# BTC Price\n",
    "# .\n",
    "# Final BTC Price=ARIMA-GARCH Forecast+ΔBTC Price.\n",
    "# Output:\n",
    "\n",
    "# The final 7-day BTC price forecast, along with updated synthetic features and technical indicators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib\n",
    "from hyperparameter_tuning import tune_hyperparameters, lstm_model_builder\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from statsmodels.tsa.vector_ar.var_model import VAR\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, root_mean_squared_error\n",
    "from arima_garch import fit_sarima"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "df = pd.read_csv(\"../data/final/dataset.csv\", parse_dates=[\"Date\"], index_col=\"Date\")\n",
    "test_pca_df = pd.read_csv(\"../data/final/test_pca_df.csv\", parse_dates=[\"Date\"], index_col=\"Date\")\n",
    "test_residuals_df = pd.read_csv(\"../data/final/test_residuals_df.csv\", parse_dates=[\"Date\"], index_col=\"Date\")\n",
    "test_arima_garch_pred = test_residuals_df[\"SARIMA-GARCH Prediction\"]\n",
    "test_residual = test_residuals_df[\"Residuals\"]\n",
    "\n",
    "scaler = joblib.load(\"../models/residual_scaler.pkl\")\n",
    "test_residual_scaled = scaler.transform(test_residual.values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7-days residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating future predictions for LSTM...\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 215ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
      "Future predictions for LSTM: [1.1475611 1.2598829 1.327847  1.3718042 1.4148743 1.4621227 1.5024335]\n",
      "Generating future predictions for BiLSTM...\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 285ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step\n",
      "Future predictions for BiLSTM: [1.775028  1.9981897 2.2799032 2.6360328 3.0585694 3.5376086 4.040752 ]\n",
      "Generating future predictions for Attention-LSTM...\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 231ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
      "Future predictions for Attention-LSTM: [1.3347352 1.437381  1.5810843 1.7251589 1.8236086 1.8609061 1.8638792]\n",
      "Generating future predictions for Attention-BiLSTM...\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 334ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step\n",
      "Future predictions for Attention-BiLSTM: [1.7432141 1.8043451 1.8100708 1.8488948 1.883738  1.9241166 1.9619312]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LSTM</th>\n",
       "      <th>BiLSTM</th>\n",
       "      <th>Attention-LSTM</th>\n",
       "      <th>Attention-BiLSTM</th>\n",
       "      <th>Ensemble</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2024-11-17 00:00:00+00:00</th>\n",
       "      <td>1.147561</td>\n",
       "      <td>1.775028</td>\n",
       "      <td>1.334735</td>\n",
       "      <td>1.743214</td>\n",
       "      <td>1.500135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-11-18 00:00:00+00:00</th>\n",
       "      <td>1.259883</td>\n",
       "      <td>1.998190</td>\n",
       "      <td>1.437381</td>\n",
       "      <td>1.804345</td>\n",
       "      <td>1.624950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-11-19 00:00:00+00:00</th>\n",
       "      <td>1.327847</td>\n",
       "      <td>2.279903</td>\n",
       "      <td>1.581084</td>\n",
       "      <td>1.810071</td>\n",
       "      <td>1.749726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-11-20 00:00:00+00:00</th>\n",
       "      <td>1.371804</td>\n",
       "      <td>2.636033</td>\n",
       "      <td>1.725159</td>\n",
       "      <td>1.848895</td>\n",
       "      <td>1.895473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-11-21 00:00:00+00:00</th>\n",
       "      <td>1.414874</td>\n",
       "      <td>3.058569</td>\n",
       "      <td>1.823609</td>\n",
       "      <td>1.883738</td>\n",
       "      <td>2.045197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-11-22 00:00:00+00:00</th>\n",
       "      <td>1.462123</td>\n",
       "      <td>3.537609</td>\n",
       "      <td>1.860906</td>\n",
       "      <td>1.924117</td>\n",
       "      <td>2.196188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-11-23 00:00:00+00:00</th>\n",
       "      <td>1.502434</td>\n",
       "      <td>4.040752</td>\n",
       "      <td>1.863879</td>\n",
       "      <td>1.961931</td>\n",
       "      <td>2.342249</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               LSTM    BiLSTM  Attention-LSTM  \\\n",
       "2024-11-17 00:00:00+00:00  1.147561  1.775028        1.334735   \n",
       "2024-11-18 00:00:00+00:00  1.259883  1.998190        1.437381   \n",
       "2024-11-19 00:00:00+00:00  1.327847  2.279903        1.581084   \n",
       "2024-11-20 00:00:00+00:00  1.371804  2.636033        1.725159   \n",
       "2024-11-21 00:00:00+00:00  1.414874  3.058569        1.823609   \n",
       "2024-11-22 00:00:00+00:00  1.462123  3.537609        1.860906   \n",
       "2024-11-23 00:00:00+00:00  1.502434  4.040752        1.863879   \n",
       "\n",
       "                           Attention-BiLSTM  Ensemble  \n",
       "2024-11-17 00:00:00+00:00          1.743214  1.500135  \n",
       "2024-11-18 00:00:00+00:00          1.804345  1.624950  \n",
       "2024-11-19 00:00:00+00:00          1.810071  1.749726  \n",
       "2024-11-20 00:00:00+00:00          1.848895  1.895473  \n",
       "2024-11-21 00:00:00+00:00          1.883738  2.045197  \n",
       "2024-11-22 00:00:00+00:00          1.924117  2.196188  \n",
       "2024-11-23 00:00:00+00:00          1.961931  2.342249  "
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_dir = \"../models\"\n",
    "look_back = 20  # Number of days to look back for LSTM models\n",
    "\n",
    "future_days = 7  # Number of days to predict\n",
    "future_dates = pd.date_range(test_pca_df.index[-1], periods=future_days + 1, freq=\"D\")[1:]\n",
    "\n",
    "# Dictionary to store future predictions\n",
    "final_residuals = {}\n",
    "\n",
    "for model_type in [\"LSTM\", \"BiLSTM\", \"Attention-LSTM\", \"Attention-BiLSTM\"]:\n",
    "    print(f\"Generating future predictions for {model_type}...\")\n",
    "\n",
    "    # Load the best model and parameters\n",
    "    model_file = f\"{model_dir}/{model_type}_best_model.pkl\"\n",
    "    param_file = f\"{model_dir}/{model_type}_best_params.pkl\"\n",
    "\n",
    "    best_model = joblib.load(model_file)\n",
    "    best_params = joblib.load(param_file)\n",
    "\n",
    "    # Access the underlying Keras model\n",
    "    keras_model = best_model.model_\n",
    "\n",
    "    # Start with the last `look_back` days of residuals from the test set\n",
    "    input_sequence = test_residual_scaled[-look_back:].reshape(1, look_back, 1)\n",
    "    future_residuals = []  # To store predicted residuals\n",
    "\n",
    "    for _ in range(future_days):\n",
    "        # Predict the next residual\n",
    "        next_residual = keras_model.predict(input_sequence)[0, 0]\n",
    "        future_residuals.append(next_residual)\n",
    "\n",
    "        # Update input sequence by appending the predicted residual\n",
    "        next_residual = np.array([[next_residual]])  # Reshape to (1, 1)\n",
    "        input_sequence = np.append(input_sequence[:, 1:, :], next_residual[:, np.newaxis, :], axis=1)\n",
    "\n",
    "    # Inverse transform predicted residuals\n",
    "    future_residuals = np.array(future_residuals).reshape(-1, 1)\n",
    "    future_residuals_inverse = scaler.inverse_transform(future_residuals).flatten()\n",
    "\n",
    "\n",
    "    # Store the future residual\n",
    "    final_residuals[model_type] = np.exp(future_residuals_inverse) - 1  # Undo log transform\n",
    "\n",
    "    print(f\"Future predictions for {model_type}: {final_residuals[model_type]}\")\n",
    "\n",
    "# Ensemble future prediction\n",
    "ensemble_future_pred = np.mean([final_residuals[model_type] for model_type in final_residuals], axis=0)\n",
    "final_residuals[\"Ensemble\"] = ensemble_future_pred\n",
    "\n",
    "# Convert future predictions to a DataFrame for visualization\n",
    "final_residual_df = pd.DataFrame(final_residuals, index=future_dates)\n",
    "\n",
    "final_residual_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Synthetic Feature Indicator Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to forecast using ARIMA\n",
    "def fit_arima(series, steps=7, order=(5, 1, 0)):\n",
    "    model = ARIMA(series, order=order)  # Adjust (p, d, q) as needed\n",
    "    model_fit = model.fit()\n",
    "    # Get predictions for the entire series\n",
    "    series_pred = model_fit.predict(start=0, end=len(series) - 1)\n",
    "    forecast = model_fit.forecast(steps=steps)\n",
    "    return series_pred, forecast\n",
    "\n",
    "# def forecast_sarima(series, steps=7, order=(1, 1, 2), seasonal_order=(1, 1, 1, 7)):\n",
    "#     # Fit the SARIMA model\n",
    "#     model = SARIMAX(series, order=order, freq='D', seasonal_order=seasonal_order)\n",
    "#     model_fit = model.fit(disp=False)\n",
    "\n",
    "#     # Get predictions for the entire series\n",
    "#     series_pred = model_fit.predict(start=0, end=len(series) - 1)\n",
    "#     # Forecast future values\n",
    "#     forecast = model_fit.forecast(steps=steps)\n",
    "#     return series_pred, forecast\n",
    "\n",
    "# Function to forecast using VAR for interdependent features\n",
    "def forecast_var(data, steps=7):\n",
    "    model = VAR(data)\n",
    "    model_fit = model.fit(maxlags=15, ic='aic')  # Adjust lag selection as needed\n",
    "    lag_order = model_fit.k_ar # get optimal lag order\n",
    "    forecast_input = data.values[-lag_order:]  # Extract the last lag_order observations\n",
    "    forecast = model_fit.forecast(y=forecast_input, steps=steps)\n",
    "    return pd.DataFrame(forecast, columns=data.columns)\n",
    "\n",
    "# Evaluate forecast\n",
    "def evaluate_forecast(train_data, sarima_pred):\n",
    "    # Ensure input is numpy array for consistency\n",
    "    train_data = np.array(train_data)\n",
    "    sarima_pred = np.array(sarima_pred)\n",
    "\n",
    "    # Check if lengths match\n",
    "    if len(train_data) != len(sarima_pred):\n",
    "        raise ValueError(\"Length of train_data and sarima_pred must be the same\")\n",
    "\n",
    "    rmse = root_mean_squared_error(train_data, sarima_pred)\n",
    "    mae = mean_absolute_error(train_data, sarima_pred)\n",
    "    mape = np.mean(np.abs((train_data - sarima_pred) / (train_data + 1e-10))) * 100\n",
    "\n",
    "    return {\"RMSE\": rmse,\"MAE\": mae,\"MAPE\": mape}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load SARIMA parameters from JSON\n",
    "with open('../results/metrics/sarima_params.json', 'r') as f:\n",
    "    sarima_params = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the list of technical indicators\n",
    "technical_features = [\n",
    "    'btc_sma_14',\n",
    "    'btc_ema_14', 'btc_rsi_14', 'btc_macd', 'btc_macd_signal',\n",
    "    'btc_macd_diff', 'btc_bb_high', 'btc_bb_low', 'btc_bb_mid',\n",
    "    'btc_bb_width', 'btc_atr_14', 'btc_trading_volume',\n",
    "    'btc_volatility_index'\n",
    "]\n",
    "\n",
    "non_technical_features = [col for col in df.columns if col not in [technical_features, 'btc_close']]\n",
    "\n",
    "train_data = df[non_technical_features].copy()\n",
    "\n",
    "# Initialize a dictionary to store scalers for each column\n",
    "column_scalers = {}\n",
    "\n",
    "# Create an empty DataFrame to store scaled data\n",
    "scaled_data = pd.DataFrame(index=train_data.index, columns=non_technical_features)\n",
    "\n",
    "# Fit a scaler for each column and transform the data\n",
    "for col in non_technical_features:\n",
    "    scaler = StandardScaler()\n",
    "    scaled_data[col] = scaler.fit_transform(train_data[col].values.reshape(-1, 1)).flatten()\n",
    "    column_scalers[col] = scaler  # Save the scaler for later use\n",
    "\n",
    "synthetic_features = pd.DataFrame(index=pd.date_range(start=df.index[-1] + pd.Timedelta(days=1), periods=7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/binnu/miniforge3/envs/myenv/lib/python3.10/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency D will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "/Users/binnu/miniforge3/envs/myenv/lib/python3.10/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency D will be used.\n",
      "  self._init_dates(dates, freq)\n"
     ]
    }
   ],
   "source": [
    "# Sentiment Indicators (arima/sarima)\n",
    "sentiment_indicator = 'google_trends_bitcoin'\n",
    "sentiment_param = sarima_params[sentiment_indicator]\n",
    "\n",
    "y_train = scaled_data[sentiment_indicator]\n",
    "\n",
    "sarima_results = fit_sarima(y_train, order=tuple(sentiment_param['order']), seasonal_order=tuple(sentiment_param['seasonal_order']))\n",
    "\n",
    "sarima_forecast = sarima_results['forecast'](steps=7)\n",
    "\n",
    "metric = evaluate_forecast(y_train, sarima_results['train_predictions'])\n",
    "\n",
    "synthetic_features[sentiment_indicator] = column_scalers[sentiment_indicator].inverse_transform(sarima_forecast.values.reshape(-1,1)).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/binnu/miniforge3/envs/myenv/lib/python3.10/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency D will be used.\n",
      "  self._init_dates(dates, freq)\n"
     ]
    }
   ],
   "source": [
    "# Blockchain Indicators (var)\n",
    "blockchain_indicator = ['active_addresses_blockchain', 'hash_rate_blockchain', 'miner_revenue_blockchain']\n",
    "\n",
    "# Forecast VAR features\n",
    "var_forecasts = forecast_var(scaled_data[blockchain_indicator], steps=7)\n",
    "\n",
    "for indicator in blockchain_indicator:\n",
    "    synthetic_features[indicator] = column_scalers[indicator].inverse_transform(var_forecasts[indicator].values.reshape(-1,1)).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Macro/Market Indicators (ARIMA/SARIMA)\n",
    "macro_indicator = [\n",
    "    'Gold_Close', 'Oil_Close', 'DJI', 'GSPC',\n",
    "    'IXIC', 'NYSE FANG+', 'ARK Innovation ETF', 'CBOE Volatility Index',\n",
    "    'iShares MSCI Emerging Markets ETF', 'Shanghai Composite Index',\n",
    "    'USD Index (DXY)', 'EUR to USD Exchange Rate'\n",
    "]\n",
    "\n",
    "# Dictionary to store SARIMA results and metrics\n",
    "sarima_results = {}\n",
    "metrics = {}\n",
    "\n",
    "# Loop through each macro indicator\n",
    "for indicator in macro_indicator:\n",
    "    # Extract SARIMA parameters for the current indicator from sarima_params\n",
    "    if indicator in sarima_params:\n",
    "        order = tuple(sarima_params[indicator]['order'])\n",
    "        seasonal_order = tuple(sarima_params[indicator]['seasonal_order'])\n",
    "    else:\n",
    "        raise ValueError(f\"SARIMA parameters for {indicator} not found in sarima_params.\")\n",
    "    \n",
    "    # Extract training data for the current indicator\n",
    "    y_train = scaled_data[indicator]\n",
    "    \n",
    "    # Fit SARIMA model\n",
    "    sarima_result = fit_sarima(y_train, order=order, seasonal_order=seasonal_order)\n",
    "    \n",
    "    # Forecast for the next 7 steps\n",
    "    sarima_forecast = sarima_result['forecast'](steps=7)\n",
    "    \n",
    "    # Evaluate the forecast\n",
    "    metric = evaluate_forecast(y_train, sarima_result['train_predictions'])\n",
    "    \n",
    "    # Inverse transform the forecast using the corresponding scaler\n",
    "    inverse_forecast = column_scalers[indicator].inverse_transform(sarima_forecast.values.reshape(-1, 1)).flatten()\n",
    "    \n",
    "    # Store results\n",
    "    sarima_results[indicator] = sarima_result\n",
    "    metrics[indicator] = metric\n",
    "    \n",
    "    # Assign the inverse-transformed forecast to synthetic_features\n",
    "    synthetic_features[indicator] = inverse_forecast\n",
    "\n",
    "# Print metrics for evaluation\n",
    "for indicator, metric in metrics.items():\n",
    "    print(f\"Metrics for {indicator}: {metric}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict First Date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating future predictions for LSTM...\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 210ms/step\n",
      "Future predictions for LSTM: [68883.67908539]\n",
      "Generating future predictions for BiLSTM...\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 295ms/step\n",
      "Future predictions for BiLSTM: [89010.17242435]\n",
      "Generating future predictions for Attention-LSTM...\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 237ms/step\n",
      "Future predictions for Attention-LSTM: [74887.43709936]\n",
      "Generating future predictions for Attention-BiLSTM...\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 352ms/step\n",
      "Future predictions for Attention-BiLSTM: [87989.71142955]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LSTM</th>\n",
       "      <th>BiLSTM</th>\n",
       "      <th>Attention-LSTM</th>\n",
       "      <th>Attention-BiLSTM</th>\n",
       "      <th>Ensemble</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2024-11-17 00:00:00+00:00</th>\n",
       "      <td>68883.679085</td>\n",
       "      <td>89010.172424</td>\n",
       "      <td>74887.437099</td>\n",
       "      <td>87989.71143</td>\n",
       "      <td>80192.75001</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   LSTM        BiLSTM  Attention-LSTM  \\\n",
       "2024-11-17 00:00:00+00:00  68883.679085  89010.172424    74887.437099   \n",
       "\n",
       "                           Attention-BiLSTM     Ensemble  \n",
       "2024-11-17 00:00:00+00:00       87989.71143  80192.75001  "
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_dir = \"../models\"\n",
    "look_back = 20  # Number of days to look back for LSTM models\n",
    "\n",
    "future_days = 1  # Number of days to predict\n",
    "future_dates = pd.date_range(test_pca_df.index[-1], periods=future_days + 1, freq=\"D\")[1:]\n",
    "\n",
    "# Dictionary to store future predictions\n",
    "future_predictions = {}\n",
    "\n",
    "for model_type in [\"LSTM\", \"BiLSTM\", \"Attention-LSTM\", \"Attention-BiLSTM\"]:\n",
    "    print(f\"Generating future predictions for {model_type}...\")\n",
    "\n",
    "    # Load the best model and parameters\n",
    "    model_file = f\"{model_dir}/{model_type}_best_model.pkl\"\n",
    "    param_file = f\"{model_dir}/{model_type}_best_params.pkl\"\n",
    "\n",
    "    best_model = joblib.load(model_file)\n",
    "    best_params = joblib.load(param_file)\n",
    "\n",
    "    # Access the underlying Keras model\n",
    "    keras_model = best_model.model_\n",
    "\n",
    "    # Start with the last `look_back` days of residuals from the test set\n",
    "    input_sequence = test_residual_scaled[-look_back:].reshape(1, look_back, 1)\n",
    "    future_residuals = []  # To store predicted residuals\n",
    "\n",
    "    for _ in range(future_days):\n",
    "        # Predict the next residual\n",
    "        next_residual = keras_model.predict(input_sequence)[0, 0]\n",
    "        future_residuals.append(next_residual)\n",
    "\n",
    "        # Update input sequence by appending the predicted residual\n",
    "        next_residual = np.array([[next_residual]])  # Reshape to (1, 1)\n",
    "        input_sequence = np.append(input_sequence[:, 1:, :], next_residual[:, np.newaxis, :], axis=1)\n",
    "\n",
    "    # Inverse transform predicted residuals\n",
    "    future_residuals = np.array(future_residuals).reshape(-1, 1)\n",
    "    future_residuals_inverse = scaler.inverse_transform(future_residuals).flatten()\n",
    "\n",
    "    # Combine ARIMA-GARCH predictions and LSTM residuals for final forecast\n",
    "    arima_garch_future_pred = test_arima_garch_pred.iloc[-1]  # Start from the last ARIMA-GARCH prediction\n",
    "    final_future_forecast_list = []\n",
    "    for residual in future_residuals_inverse:\n",
    "        final_future_forecast = arima_garch_future_pred + residual\n",
    "        final_future_forecast_list.append(final_future_forecast)\n",
    "\n",
    "    # Store the future predictions\n",
    "    future_predictions[model_type] = np.exp(final_future_forecast_list) - 1  # Undo log transform\n",
    "\n",
    "    print(f\"Future predictions for {model_type}: {future_predictions[model_type]}\")\n",
    "\n",
    "# Ensemble future prediction\n",
    "ensemble_future_pred = np.mean([future_predictions[model_type] for model_type in future_predictions], axis=0)\n",
    "future_predictions[\"Ensemble\"] = ensemble_future_pred\n",
    "\n",
    "# Convert future predictions to a DataFrame for visualization\n",
    "future_predictions_df = pd.DataFrame(future_predictions, index=future_dates)\n",
    "\n",
    "future_predictions_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "btc_price = pd.read_csv(\"../data/raw/historical_data.csv\", parse_dates=[\"Date\"], index_col=\"Date\")\n",
    "\n",
    "# Extract high (High), low (Low), and volume (Volume) columns\n",
    "high_low_vol = ['High', 'Low', 'Volume']\n",
    "\n",
    "btc_train = btc_price[high_low_vol].copy()\n",
    "\n",
    "# Initialize a dictionary to store scalers for each column\n",
    "btc_scalers = {}\n",
    "\n",
    "# Create an empty DataFrame to store scaled data\n",
    "scaled_btc = pd.DataFrame(index=btc_train.index, columns=high_low_vol)\n",
    "\n",
    "# Fit a scaler for each column and transform the data\n",
    "for col in high_low_vol:\n",
    "    scaler = StandardScaler()\n",
    "    scaled_btc[col] = scaler.fit_transform(btc_train[col].values.reshape(-1, 1)).flatten()\n",
    "    btc_scalers[col] = scaler  # Save the scaler for later use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/binnu/miniforge3/envs/myenv/lib/python3.10/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency D will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "/Users/binnu/miniforge3/envs/myenv/lib/python3.10/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency D will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "/Users/binnu/miniforge3/envs/myenv/lib/python3.10/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency D will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "/Users/binnu/miniforge3/envs/myenv/lib/python3.10/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency D will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "/Users/binnu/miniforge3/envs/myenv/lib/python3.10/site-packages/statsmodels/base/model.py:607: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "/Users/binnu/miniforge3/envs/myenv/lib/python3.10/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency D will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "/Users/binnu/miniforge3/envs/myenv/lib/python3.10/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency D will be used.\n",
      "  self._init_dates(dates, freq)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics for High: {'RMSE': 0.04054129733236314, 'MAE': 0.018541818588258083, 'MAPE': 20.105934102653226}\n",
      "Metrics for Low: {'RMSE': 0.04767266205687547, 'MAE': 0.02128387967773765, 'MAPE': 6.6521278132224495}\n",
      "Metrics for Volume: {'RMSE': 0.4461574468890595, 'MAE': 0.1892674912479586, 'MAPE': 149.549605857712}\n"
     ]
    }
   ],
   "source": [
    "# # Dictionary to store SARIMA results and metrics\n",
    "# sarima_results = {}\n",
    "# metrics = {}\n",
    "high_low_vol_forecast = pd.DataFrame(index=pd.date_range(start=df.index[-1] + pd.Timedelta(days=1), periods=7))\n",
    "\n",
    "# Loop through each macro indicator\n",
    "for indicator in high_low_vol:\n",
    "    # Extract SARIMA parameters for the current indicator from sarima_params\n",
    "    if indicator in sarima_params:\n",
    "        order = tuple(sarima_params[indicator]['order'])\n",
    "        seasonal_order = tuple(sarima_params[indicator]['seasonal_order'])\n",
    "    else:\n",
    "        raise ValueError(f\"SARIMA parameters for {indicator} not found in sarima_params.\")\n",
    "    \n",
    "    # Extract training data for the current indicator\n",
    "    y_train = scaled_btc[indicator]\n",
    "    \n",
    "    # Fit SARIMA model\n",
    "    sarima_result = fit_sarima(y_train, order=order, seasonal_order=seasonal_order)\n",
    "    \n",
    "    # Forecast for the next 7 steps\n",
    "    sarima_forecast = sarima_result['forecast'](steps=7)\n",
    "    \n",
    "    # Evaluate the forecast\n",
    "    metric = evaluate_forecast(y_train, sarima_result['train_predictions'])\n",
    "    \n",
    "    # Inverse transform the forecast using the corresponding scaler\n",
    "    inverse_forecast = btc_scalers[indicator].inverse_transform(sarima_forecast.values.reshape(-1, 1)).flatten()\n",
    "    \n",
    "    # Store results\n",
    "    sarima_results[indicator] = sarima_result\n",
    "    metrics[indicator] = metric\n",
    "    \n",
    "    # Assign the inverse-transformed forecast to synthetic_features\n",
    "    high_low_vol_forecast[indicator] = inverse_forecast\n",
    "\n",
    "# Print metrics for evaluation\n",
    "for indicator, metric in metrics.items():\n",
    "    print(f\"Metrics for {indicator}: {metric}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2024-11-17 00:00:00+00:00</th>\n",
       "      <td>91743.122097</td>\n",
       "      <td>91049.717734</td>\n",
       "      <td>7.795624e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-11-18 00:00:00+00:00</th>\n",
       "      <td>91740.882586</td>\n",
       "      <td>90869.459341</td>\n",
       "      <td>7.453924e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-11-19 00:00:00+00:00</th>\n",
       "      <td>91740.562937</td>\n",
       "      <td>90789.099831</td>\n",
       "      <td>8.096063e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-11-20 00:00:00+00:00</th>\n",
       "      <td>91740.517314</td>\n",
       "      <td>90834.314226</td>\n",
       "      <td>8.153935e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-11-21 00:00:00+00:00</th>\n",
       "      <td>91740.510802</td>\n",
       "      <td>90794.706787</td>\n",
       "      <td>8.252113e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-11-22 00:00:00+00:00</th>\n",
       "      <td>91740.509872</td>\n",
       "      <td>90888.600434</td>\n",
       "      <td>7.706750e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-11-23 00:00:00+00:00</th>\n",
       "      <td>91740.509739</td>\n",
       "      <td>90955.787994</td>\n",
       "      <td>7.565415e+10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   High           Low        Volume\n",
       "2024-11-17 00:00:00+00:00  91743.122097  91049.717734  7.795624e+10\n",
       "2024-11-18 00:00:00+00:00  91740.882586  90869.459341  7.453924e+10\n",
       "2024-11-19 00:00:00+00:00  91740.562937  90789.099831  8.096063e+10\n",
       "2024-11-20 00:00:00+00:00  91740.517314  90834.314226  8.153935e+10\n",
       "2024-11-21 00:00:00+00:00  91740.510802  90794.706787  8.252113e+10\n",
       "2024-11-22 00:00:00+00:00  91740.509872  90888.600434  7.706750e+10\n",
       "2024-11-23 00:00:00+00:00  91740.509739  90955.787994  7.565415e+10"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "high_low_vol_forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Technical Indicators (calculated from forecasted btc_price)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA transform future indicators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### PCA transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ARIMA-GARCH Forecasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# arima forecast\n",
    "\n",
    "# extract residual to fit into garch\n",
    "\n",
    "# garch forecast\n",
    "\n",
    "# combine into arima-garch forecast\n",
    "\n",
    "# residual = btc_price - arima-garch forecast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM Forecasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use future residual from arima-garch forecast to predict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequence Learning ???"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
