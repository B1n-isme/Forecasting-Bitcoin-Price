{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# Define the tickers and their descriptions\n",
    "tickers = {\n",
    "    \"^GSPC\": \"S&P 500\",\n",
    "    \"^IXIC\": \"NASDAQ\",\n",
    "    \"^DJI\": \"Dow Jones\",\n",
    "    \"^NYFANG\": \"NYSE FANG+\",\n",
    "    \"ARKK\": \"ARK Innovation ETF\",\n",
    "    \"^VIX\": \"CBOE Volatility Index\",\n",
    "    \"EEM\": \"iShares MSCI Emerging Markets ETF\",\n",
    "    \"000001.SS\": \"Shanghai Composite Index\",\n",
    "    \"DX-Y.NYB\": \"USD Index (DXY)\",\n",
    "    \"EURUSD=X\": \"EUR to USD Exchange Rate\"\n",
    "}\n",
    "\n",
    "# Set the date range\n",
    "start_date = \"2014-11-14\"\n",
    "end_date = \"2024-11-18\"\n",
    "\n",
    "# Fetch data and save to individual CSV files\n",
    "for ticker, description in tickers.items():\n",
    "    print(f\"Fetching data for {description} ({ticker})...\")\n",
    "    data = yf.download(ticker, start=start_date, end=end_date)\n",
    "    if not data.empty:\n",
    "        # Save with both ticker and description in the filename\n",
    "        file_name = f\"{ticker}_{description.replace(' ', '_').replace('+', '').replace('/', '_')}.csv\"\n",
    "        data.to_csv(f\"../data/raw/{file_name}\")\n",
    "        print(f\"Saved {description} data to {file_name}\")\n",
    "    else:\n",
    "        print(f\"No data found for {description} ({ticker})\")\n",
    "\n",
    "print(\"Data fetching complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully combined and saved to 'full_combined_data.csv'\n"
     ]
    }
   ],
   "source": [
    "# import pandas as pd\n",
    "# from statsmodels.tsa.arima.model import ARIMA\n",
    "\n",
    "# # Helper function for ARIMA-based imputation\n",
    "# def arima_impute(series):\n",
    "#     if series.isnull().sum() > 0:  # Apply ARIMA only if there are still missing values\n",
    "#         model = ARIMA(series, order=(1, 1, 1))  # Adjust (p, d, q) as needed\n",
    "#         fitted_model = model.fit()\n",
    "#         series = series.fillna(fitted_model.fittedvalues)\n",
    "#     return series\n",
    "\n",
    "# # Load the cleaned data for existing indices\n",
    "# dji_data = pd.read_csv(\"^DJI_data.csv\", parse_dates=[\"Date\"], index_col=\"Date\")\n",
    "# gspc_data = pd.read_csv(\"^GSPC_data.csv\", parse_dates=[\"Date\"], index_col=\"Date\")\n",
    "# ixic_data = pd.read_csv(\"^IXIC_data.csv\", parse_dates=[\"Date\"], index_col=\"Date\")\n",
    "# btc_data = pd.read_csv(\"BTC-USD_data.csv\", parse_dates=[\"Date\"], index_col=\"Date\")\n",
    "\n",
    "# # Load the cleaned data for new indicators\n",
    "# nyfang_data = pd.read_csv(\"NYSE_FANG.csv\", parse_dates=[\"Date\"], index_col=\"Date\")\n",
    "# arkk_data = pd.read_csv(\"ARK_Innovation_ETF.csv\", parse_dates=[\"Date\"], index_col=\"Date\")\n",
    "# vix_data = pd.read_csv(\"CBOE_Volatility_Index.csv\", parse_dates=[\"Date\"], index_col=\"Date\")\n",
    "# eem_data = pd.read_csv(\"iShares_MSCI_Emerging_Markets_ETF.csv\", parse_dates=[\"Date\"], index_col=\"Date\")\n",
    "# shanghai_data = pd.read_csv(\"Shanghai_Composite_Index.csv\", parse_dates=[\"Date\"], index_col=\"Date\")\n",
    "# dxy_data = pd.read_csv(\"USD_Index_(DXY).csv\", parse_dates=[\"Date\"], index_col=\"Date\")\n",
    "# eurusd_data = pd.read_csv(\"EUR_to_USD_Exchange_Rate.csv\", parse_dates=[\"Date\"], index_col=\"Date\")\n",
    "\n",
    "# # Retain only the Close prices and rename columns\n",
    "# dji_data_cleaned = dji_data[\"Close\"].to_frame().rename(columns={\"Close\": \"DJI\"})\n",
    "# gspc_data_cleaned = gspc_data[\"Close\"].to_frame().rename(columns={\"Close\": \"GSPC\"})\n",
    "# ixic_data_cleaned = ixic_data[\"Close\"].to_frame().rename(columns={\"Close\": \"IXIC\"})\n",
    "# btc_data_cleaned = btc_data[\"Close\"].to_frame().rename(columns={\"Close\": \"BTC\"})\n",
    "\n",
    "# # Retain only the Close prices and rename columns\n",
    "# nyfang_data_cleaned = nyfang_data[\"Close\"].to_frame().rename(columns={\"Close\": \"NYSE FANG+\"})\n",
    "# arkk_data_cleaned = arkk_data[\"Close\"].to_frame().rename(columns={\"Close\": \"ARK Innovation ETF\"})\n",
    "# vix_data_cleaned = vix_data[\"Close\"].to_frame().rename(columns={\"Close\": \"CBOE Volatility Index\"})\n",
    "# eem_data_cleaned = eem_data[\"Close\"].to_frame().rename(columns={\"Close\": \"iShares MSCI Emerging Markets ETF\"})\n",
    "# shanghai_data_cleaned = shanghai_data[\"Close\"].to_frame().rename(columns={\"Close\": \"Shanghai Composite Index\"})\n",
    "# dxy_data_cleaned = dxy_data[\"Close\"].to_frame().rename(columns={\"Close\": \"USD Index (DXY)\"})\n",
    "# eurusd_data_cleaned = eurusd_data[\"Close\"].to_frame().rename(columns={\"Close\": \"EUR to USD Exchange Rate\"})\n",
    "\n",
    "# # Combine all datasets using an outer join on Date\n",
    "# combined_data = pd.concat([\n",
    "#     dji_data_cleaned, gspc_data_cleaned, ixic_data_cleaned, btc_data_cleaned,\n",
    "#     nyfang_data_cleaned, arkk_data_cleaned, vix_data_cleaned,\n",
    "#     eem_data_cleaned, shanghai_data_cleaned, dxy_data_cleaned, eurusd_data_cleaned\n",
    "# ], axis=1, join=\"outer\")\n",
    "\n",
    "# # Forward fill missing values\n",
    "# combined_data.ffill(inplace=True)\n",
    "\n",
    "# # Save the combined DataFrame to a CSV file\n",
    "# combined_data.to_csv(\"macro_financial_factors.csv\")\n",
    "\n",
    "# print(\"Data successfully combined and saved to 'macro_financial_factors.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully combined and saved to 'macro_financial_factors.csv'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/pf/4hxrldrx2g7g5x19dwlvk36w0000gn/T/ipykernel_10724/1121476715.py:41: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  combined_data = combined_data.fillna(method=\"ffill\")\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "\n",
    "# Helper function for ARIMA-based imputation\n",
    "def arima_impute(series):\n",
    "    if series.isnull().sum() > 0 and series.notnull().sum() > 5:  # Ensure enough valid points\n",
    "        model = ARIMA(series, order=(1, 1, 1))  # Adjust (p, d, q) as needed\n",
    "        fitted_model = model.fit()\n",
    "        series = series.fillna(fitted_model.fittedvalues)\n",
    "    return series\n",
    "\n",
    "# Load datasets\n",
    "datasets = {\n",
    "    \"DJI\": \"^DJI_Dow_Jones.csv\",\n",
    "    \"GSPC\": \"^GSPC_S&P_500.csv\",\n",
    "    \"IXIC\": \"^IXIC_NASDAQ.csv\",\n",
    "    \"NYSE FANG+\": \"^NYFANG_NYSE_FANG+.csv\",\n",
    "    \"ARK Innovation ETF\": \"ARKK_ARK_Innovation_ETF.csv\",\n",
    "    \"CBOE Volatility Index\": \"^VIX_CBOE_Volatility_Index.csv\",\n",
    "    \"iShares MSCI Emerging Markets ETF\": \"EEM_iShares_MSCI_Emerging_Markets_ETF.csv\",\n",
    "    \"Shanghai Composite Index\": \"000001.SS_Shanghai_Composite_Index.csv\",\n",
    "    \"USD Index (DXY)\": \"DX-Y.NYB_USD_Index_(DXY).csv\",\n",
    "    \"EUR to USD Exchange Rate\": \"EURUSD=X_EUR_to_USD_Exchange_Rate.csv\",\n",
    "}\n",
    "\n",
    "# Read datasets and retain only 'Close' prices, renaming appropriately\n",
    "dataframes = []\n",
    "for name, filepath in datasets.items():\n",
    "    df = pd.read_csv(f\"../data/raw/{filepath}\", parse_dates=[\"Date\"], index_col=\"Date\")\n",
    "    df_cleaned = df[\"Close\"].to_frame(name=name)  # Retain only 'Close' column and rename\n",
    "    dataframes.append(df_cleaned)\n",
    "\n",
    "# Combine all datasets using an outer join\n",
    "combined_data = pd.concat(dataframes, axis=1, join=\"outer\")\n",
    "\n",
    "# Step 1: Create a full date range for the combined data\n",
    "full_date_range = pd.date_range(start=combined_data.index.min(), end=combined_data.index.max())\n",
    "combined_data = combined_data.reindex(full_date_range)\n",
    "\n",
    "# Step 2: Forward Fill for Edge Cases\n",
    "combined_data = combined_data.fillna(method=\"ffill\")\n",
    "\n",
    "# Step 3: Linear Interpolation for Small Gaps\n",
    "combined_data = combined_data.interpolate(method=\"linear\")\n",
    "\n",
    "# Step 4: ARIMA-Based Imputation for Large Gaps\n",
    "for column in combined_data.columns:\n",
    "    combined_data[column] = arima_impute(combined_data[column])\n",
    "\n",
    "# Step 5: Final Fallback for Remaining Missing Values\n",
    "combined_data = combined_data.fillna(combined_data.mean())  # Fallback strategy\n",
    "\n",
    "combined_data.index.name = \"Date\"\n",
    "\n",
    "# Save the combined DataFrame to a CSV file\n",
    "combined_data.to_csv(\"../data/processed/macro_financial_factors.csv\")\n",
    "\n",
    "print(\"Data successfully combined and saved to 'macro_financial_factors.csv'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "webscraping",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
